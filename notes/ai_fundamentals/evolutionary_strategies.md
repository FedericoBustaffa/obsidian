---
id: evolutionary_strategies
aliases: []
tags:
  - master
  - ai_fundamentals
---

# Evolutionary Strategies

The aim of **evolutionary strategies** is to optimize **continuous functions**,
starting from a pool of individuals, they are **evaluated** and then **mutated**
and **recombined**.

## Mutation

These kind of algorithms heavily rely on **mutation** in order to explore the
search space. In other words we want to modify every individual by adding random
noise, sampled from a gaussian distribution $\mathcal{N} (0, 1)$, to its
features.

At the beginning (generation $g = 0$) of the simulation a generic individual
$x^{g=0}$ is sampled from the search space and represents a candidate solution
(a vector of features).

Another part of each individual is a set of **strategy parameters** $\sigma^0$
that regulate how much that individual change (_step size_). So in total an
individual is identified by the couple $(x, \sigma)$ where $x$ is the so called
**genotype**, while $\sigma$ is a set of _endogenous_ parameters that evolve
with individual but do not represent anything for the construction of the
solution.

For the mutation we can choose either

- **Isotropic gaussian**: single distribution (and so single strategy parameter)
  for every dimension of the space
  $$x^{g+1} = x^g + \sigma \cdot \mathcal{N}(0, I)$$
  with $I$ the $n \times n$ identity matrix, where $n$ is the number of features
  of $x^g$.
- **Multiple gaussians**: one distributions (so one strategy parameters) for
  each dimension of the search space
  $$
  x^{g+1} = x^g + [\sigma_1 \cdot \mathcal{N}(0, 1), \dots, \sigma_n
  \mathcal{N}(0, 1)]
  $$
  or in a more compact form
  $$x^{g+1} = x^g + D \cdot \mathcal{N} (0, 1)$$
  with $D$ diagonal matrix containing every $\sigma_i$.

As said these endogenous parameters can evolve in the process, but actually
there are cases where keeping them constant results in a failure of the
optimization process, and so they should evolve alongside the individual.

This lead to **self-adaptive** strategy parameters, that for example start with
an high value, in order to move faster at the beginning and promote exploration,
and decrease over time, favoring exploitation and let refine what we discover
before.

## Recombination

The **recombination** phase is instead defined by

- **Mixing number** $\rho$: the number of parents to generate one offspring. In
  case $\rho = 1$ we are talking about _cloning_.
- **Offsprings number** $\lambda$: the number of offsprings generated by a group
  of $rho$ parents. This can also bee seen as selecting $\lambda$ groups of
  $\rho$ parents that generate $1$ offspring each.

The actual recombination of features can be implemented in many ways and always
depends on the problem.

## $(\mu, \lambda)$ - Strategy

The $(\mu, \lambda)$ strategy, also known as **comma strategy** starts with a
population $X^g$ and set of strategy parameters $\sigma^g$ equal for all the
features of a given individual (isotropic gaussian). Then

1. Generate $\lambda$ offsprings.
2. Select parents at random $P^g$.
3. Update the strategy values by recombination and mutation
   $$
   \sigma_k^{g+1} = \text{recombine} (\sigma^g | P^g) \cdot
   e^{\epsilon_k \sim \mathcal{N} (0, 1)}
   $$
4. Update individuals by recombination and mutation with the update strategy
   parameters
   $$x_k^{g+1} = \text{recombine} (P^g) + \sigma_k^{g+1} \cdot \mathcal{N} (0, 1)$$
5. Evaluate fitness.

The best $\mu$ offsprings are kept out of all $\lambda$ offsprings ($\lambda >
\mu$) and the parents are always discarded.

Note that $\lambda$ and $\mu$ are so called _exogenous_ parameters that are set
once and never change during evolution (hyper-parameter).

Sometimes also the mixing number $\rho$ is included in the formulation: $(\mu /
\rho, \lambda)$.

In case of $\mu = \lambda$ there is no selection because all offsprings are kept
and all parents are discarded, and so it's like a random walk in the search
space.

This strategy is usually better in exploring the search space but can be
sub-optimal because of the the possible discard of some good individual. Anyway
for _deceptive_ problems it behaves fairly well.

## $(\mu + \lambda)$ - Strategy

The $(\mu + \lambda)$ strategy is basically the same as the _comma_ strategy but
now also parents compete for survival with offsprings and as before the best
$\mu$ individuals are kept.

In this way if a parent is already the best solution that will ever be
discovered, it will survive until the end, implementing an **elitist policy** of
survival. The _comma_ strategy instead can discard a good solution and never
find it again.

Sometimes also the mixing number $\rho$ is included in the formulation: $(\mu /
\rho + \lambda)$.

In case of $\mu = \lambda$ there is no particular issue because $\lambda$
individuals are chosen out of a set of $\mu + \lambda$ individuals.

This strategy is usually better in exploiting the search space because of its
_elitism_ but can converge faster on local optima if the problem is
_deceptive_.

## Covariance Matrix Adaptation

One of the most popular evolution strategy is the **covariance matrix
adaptation** (**CMA**) which uses an **adaptive** covariance matrix to model the
instensity and direction of the mutation.

The matrix adjust itself based on the local shape of the function landscape,
speeding up on plateaus and slowing down when there are hills.

The population is sampled from a multivariate gaussian distribution

$$\mathcal{N} (m^g, C^g)$$

where $C^g$ is the covariance matrix at generation $g$, that is initialized as
the identity and where $m^g$ is the mean vector taken as candidate solution,
that is instead randomly initialized.

The basic structure of the algorithm is the following

1. Sample $\lambda$ offsprings from the initial distribution
   - $x^{g+1} = m^g + \sigma \cdot \mathcal{N} (0, C^g)$
   - $x^g \sim \mathcal{N}(m^g, \sigma^2 \cdot C^g)$
2. Sort offsprings based on increasing fitness values.
3. Offsprings update mean vector values by truncated selection ($\mu < \lambda$)
   $$m^{g+1} = \sum_{i=1}^\mu w_i \cdot x_i^{g+1}$$
   where $\mu$ is the number of offsprings selected.

The weights $w_1 \geq \dots \geq w_\mu > 0$ such that

$$\sum_{i=1}^\mu w_i = 1$$

are introduced in order to update the mean vector with a weighted sum that gives
more importance to better individuals. In this way the new center of the
distribution is shifted towards the best offsprings (not in the _middle_).

So basically we have a multivariate gaussian distribution with the same mean but
with an adaptive variance in each direction of the space. Soon or later the
distribution will point in the direction of a local optima.

## References

- [[evolutionary_optimization]]
