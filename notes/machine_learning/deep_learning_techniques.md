---
id: deep_learning_techniques
aliases: []
tags:
  - master
  - machine_learning
---

# Deep Learning Techniques

In order to train a deep neural networks standard appraches and techniques are
not enough. In the past training was done by pre-training but nowadays there are
better way to train a model end-to-end for a specific task.

The main issues with deep neural networks are related to the gradient, that can
become too small or too big depending on weights magnitude. Multiplication
through many layers can cause these issues causing the failure of training.

## Exploding Gradient

When the model backpropagate the error through many layers, when the weights are
big, the gradient magnitude grows exponentially, causing very unstable training
and overflows.

### Gradient Clipping

During the optimization, the model could encounter cliffs in the cost function,
causing a big variation in the gradient and causing big jumps in the
optimization process.

![Loss Function Cliff|600](/files/gradient_clipping.png)

In order to avoid this and mantain a smooth stable training, is possible to
**clip** the gradient if its norm exceeds a threshold $v$, so if $\| g \| > v$,
where $g$ is the gradient vector, the new gradient becomes

$$g = v \cdot \frac{g}{\| g \|}$$

Take into account that the clipping can be done layer by layer during
backpropagation or _globally_ in a single step:

- **Layer wise**: during backpropagation each layer computes its gradient
  vector and clip it if necessary. Can be finer for each layer but can modify
  proportions between layers.
- **Global**: each layer computes its gradient vector. The clipping is based on
  the norm of the global gradient vector of each layer. This is more stable and
  preserve relative gradient directions through layers.

The standard for most libraries is the global approach.

## Gradient Vanish

If weights are small can happen that the gradient for first layers is too small
and the update is almost zero. This usually happens for sigmoidal activation
functions that reach saturations and produce very small gradient.

Also repetition of multiplication of small quantities through many layers leads
to exponentially small gradients going towards input layer.

This problem can be addressed in many ways, for example with **ReLU** activation
function (and its variants) or with **batch normalization**.

### Batch Normalization

An interesting technique is called **batch normalization** at it consists in
normalizing each batch through a standardization, keeping $wx + b$ with mean to
0 and variance to 1.

This helps stabilize sigmoids activation functions that now will saturate less
easily and it adds a regularization effect due to the introduced noise.

## Dropout

Another interesting technique for deep learning is **dropout** that basically
randomly selects a subset of the network during training.

Basically a mask is applied the entire network, turning on and off some neurons
and train them for an epoch. Typically the number of **subnet** is fixed (like
10-20) and some configuration might not be valid: there isn't a path from input
to output.

![Dropout|400](/files/dropout.png)

This has the effect of training multiple models potentially with different input
parts (like a bootstrap) but for the same task. In practice this is an ensemble
(in particular a bagging) of networks that in the end make their prediction
based on a majority vote (or a mean for regression).

Of course this introduces noise and act like a regularization technique, like
for every ensemble model; high variance models can perform well on average.

Typically there is an hyperparameter that defines the probability of sampling a
mask value of 1. Each unit not included is multiplied by 0 ("drop out").

## References

- [[deep_learning]]
- [[neural_networks]]
- [[neural_networks_training]]
